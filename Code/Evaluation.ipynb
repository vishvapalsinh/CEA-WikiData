{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(df_annotated, df_true):\n",
    "    # Number of correct annotations\n",
    "    correct_annotations = (df_annotated.iloc[:, 3] == df_true.iloc[:, 3]).sum()\n",
    "    # Number of wrong annotations\n",
    "    wrong_annotations = ((df_annotated.iloc[:, 3] != df_true.iloc[:, 3]) & (df_annotated.iloc[:, 3].notna())).sum()\n",
    "    print(df_annotated[(df_annotated.iloc[:, 3] != df_true.iloc[:, 3]) & (df_annotated.iloc[:, 3].notna())])\n",
    "    # Number of empty annotations\n",
    "    empty_annotations = (df_annotated.iloc[:, 3].isna()).sum()\n",
    "    # Ground truth annotations: is the total number of targets\n",
    "    ground_truth_annotations = df_true.shape[0]\n",
    "    # Submitted annotations\n",
    "    submitted_annotations = ground_truth_annotations - empty_annotations\n",
    "    print(\"Correct Annotation\")\n",
    "    print(correct_annotations)\n",
    "    print(\"Incorrect Annotations\")\n",
    "    print(wrong_annotations)\n",
    "    print(\"Empty Annotations\")\n",
    "    print(empty_annotations)\n",
    "    precision = correct_annotations / submitted_annotations\n",
    "    recall = correct_annotations / ground_truth_annotations\n",
    "    f1_score = ( 2 * precision * recall ) / ( precision + recall )\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"f1_score: {f1_score}\")\n",
    "    \n",
    "\n",
    "# Load ground truth dataframe\n",
    "df_true = pd.read_csv(\"DataSets/Valid/gt/cea_gt.csv\", header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0  1  2                                         3\n",
      "2     BQC7DZZR  1  0    http://www.wikidata.org/entity/Q987536\n",
      "4     BQC7DZZR  3  0   http://www.wikidata.org/entity/Q7998039\n",
      "5     BQC7DZZR  4  0   http://www.wikidata.org/entity/Q5604743\n",
      "6     BQC7DZZR  5  0   http://www.wikidata.org/entity/Q2396806\n",
      "7     C8RTQNU5  1  0    http://www.wikidata.org/entity/Q925018\n",
      "...        ... .. ..                                       ...\n",
      "4182  FJJHB85J  8  0   http://www.wikidata.org/entity/Q6920614\n",
      "4184  FJJHB85J  9  0     http://www.wikidata.org/entity/Q80344\n",
      "4202  NYGNP7RK  2  0    http://www.wikidata.org/entity/Q740393\n",
      "4216  NYGNP7RK  9  0  http://www.wikidata.org/entity/Q18006151\n",
      "4238  MCYL241A  8  0    http://www.wikidata.org/entity/Q533661\n",
      "\n",
      "[1198 rows x 4 columns]\n",
      "Correct Annotation\n",
      "2817\n",
      "Incorrect Annotations\n",
      "1198\n",
      "Empty Annotations\n",
      "232\n",
      "Precision: 0.7016189290161893\n",
      "Recall: 0.6632917353425948\n",
      "f1_score: 0.681917211328976\n"
     ]
    }
   ],
   "source": [
    "# Load output dataframe for the first experiment\n",
    "df_output_first_experiment = pd.read_csv(\"DataSets/Valid/cea annotation/output_baseline.csv\", header=None)\n",
    "evaluate(df_output_first_experiment, df_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0  1  2                                          3\n",
      "34    R9VI0ZU0  2  0      http://www.wikidata.org/entity/Q34494\n",
      "35    R9VI0ZU0  2  1      http://www.wikidata.org/entity/Q36687\n",
      "36    I5QUEVR8  1  0     http://www.wikidata.org/entity/Q110049\n",
      "46    9155JW8C  9  0     http://www.wikidata.org/entity/Q482969\n",
      "51    TF13GBS9  4  0     http://www.wikidata.org/entity/Q698767\n",
      "...        ... .. ..                                        ...\n",
      "4120  1Q9AQVZ4  5  1       http://www.wikidata.org/entity/Q1460\n",
      "4126  1Q9AQVZ4  8  2     http://www.wikidata.org/entity/Q154668\n",
      "4128  1Q9AQVZ4  9  2     http://www.wikidata.org/entity/Q154668\n",
      "4137  ZB2ILR19  1  0  http://www.wikidata.org/entity/Q108111059\n",
      "4238  MCYL241A  8  0     http://www.wikidata.org/entity/Q533661\n",
      "\n",
      "[537 rows x 4 columns]\n",
      "Correct Annotation\n",
      "3478\n",
      "Incorrect Annotations\n",
      "537\n",
      "Empty Annotations\n",
      "232\n",
      "Precision: 0.8662515566625155\n",
      "Recall: 0.818931010124794\n",
      "f1_score: 0.8419268942144759\n"
     ]
    }
   ],
   "source": [
    "# Load output dataframe for the first experiment\n",
    "df_output_first_experiment = pd.read_csv(\"DataSets/Valid/cea annotation/output_with_row_context.csv\", header=None)\n",
    "evaluate(df_output_first_experiment, df_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0  1  2                                          3\n",
      "34    R9VI0ZU0  2  0      http://www.wikidata.org/entity/Q34494\n",
      "35    R9VI0ZU0  2  1      http://www.wikidata.org/entity/Q36687\n",
      "36    I5QUEVR8  1  0     http://www.wikidata.org/entity/Q110049\n",
      "46    9155JW8C  9  0     http://www.wikidata.org/entity/Q482969\n",
      "51    TF13GBS9  4  0     http://www.wikidata.org/entity/Q698767\n",
      "...        ... .. ..                                        ...\n",
      "4126  1Q9AQVZ4  8  2     http://www.wikidata.org/entity/Q154668\n",
      "4127  1Q9AQVZ4  9  0   http://www.wikidata.org/entity/Q85755960\n",
      "4128  1Q9AQVZ4  9  2     http://www.wikidata.org/entity/Q154668\n",
      "4137  ZB2ILR19  1  0  http://www.wikidata.org/entity/Q108111059\n",
      "4238  MCYL241A  8  0     http://www.wikidata.org/entity/Q533661\n",
      "\n",
      "[612 rows x 4 columns]\n",
      "Correct Annotation\n",
      "3556\n",
      "Incorrect Annotations\n",
      "612\n",
      "Empty Annotations\n",
      "79\n",
      "Precision: 0.8531669865642995\n",
      "Recall: 0.8372969154697434\n",
      "f1_score: 0.8451574569221628\n"
     ]
    }
   ],
   "source": [
    "# Load output dataframe for the first experiment\n",
    "df_output_first_experiment = pd.read_csv(\"DataSets/Valid/cea annotation/output_with_special_character_handling_and_correction.csv\", header=None)\n",
    "evaluate(df_output_first_experiment, df_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
